{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives and Their Applications in Machine Learning\n",
    "\n",
    "Just as linear algebra forms the foundation of many machine learning algorithms, derivatives are crucial for understanding how these algorithms learn and optimize. In this notebook, we'll explore various aspects of derivatives and their applications in machine learning and deep learning.\n",
    "\n",
    "## Topics Covered:\n",
    "1. Basic derivative functions\n",
    "2. Nested functions and their derivatives\n",
    "3. Chain rule implementation\n",
    "4. Derivatives with multiple inputs\n",
    "5. Derivatives with multiple vector inputs\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "## 1. Basic Derivative Functions\n",
    "\n",
    "Derivatives measure the rate of change of a function with respect to its input. In machine learning, we use derivatives to understand how changes in our model's parameters affect its performance, allowing us to optimize these parameters.\n",
    "\n",
    "Let's start with a basic quadratic function and its derivative:\n",
    "\n",
    "$$f(x) = x^2$$\n",
    "$$f'(x) = 2x$$\n",
    "\n",
    "Here's a visualization of this function and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def df(x):\n",
    "    return 2*x\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = f(x)\n",
    "dy = df(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='f(x) = x^2')\n",
    "plt.plot(x, dy, label=\"f'(x) = 2x\")\n",
    "plt.legend()\n",
    "plt.title('Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nested Functions and Their Derivatives\n",
    "\n",
    "In machine learning models, we often deal with nested functions. Understanding how to differentiate these is key to implementing backpropagation in neural networks.\n",
    "\n",
    "Let's look at a nested function and its derivative:\n",
    "\n",
    "$$g(x) = \\sin(x^2)$$\n",
    "$$g'(x) = 2x \\cos(x^2)$$\n",
    "\n",
    "Here's a visualization of this nested function and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return np.sin(x**2)\n",
    "\n",
    "def dg(x):\n",
    "    return 2*x * np.cos(x**2)\n",
    "\n",
    "x = np.linspace(-3, 3, 200)\n",
    "y = g(x)\n",
    "dy = dg(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='g(x) = sin(x^2)')\n",
    "plt.plot(x, dy, label=\"g'(x) = 2x * cos(x^2)\")\n",
    "plt.legend()\n",
    "plt.title('Nested Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain Rule Implementation\n",
    "\n",
    "The chain rule is a fundamental concept in calculus that allows us to compute the derivative of composite functions. In the context of neural networks, the chain rule is the key principle behind backpropagation.\n",
    "\n",
    "The chain rule states that for two functions $f(x)$ and $g(x)$, the derivative of their composition is:\n",
    "\n",
    "$$(f \\circ g)'(x) = f'(g(x)) \\cdot g'(x)$$\n",
    "\n",
    "Let's implement the chain rule for a simple composite function:\n",
    "\n",
    "$$h(x) = e^{x^3}$$\n",
    "$$h'(x) = 3x^2 \\cdot e^{x^3}$$\n",
    "\n",
    "Here's a visualization of this composite function and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "def inner(x):\n",
    "    return x**3\n",
    "\n",
    "def composite(x):\n",
    "    return outer(inner(x))\n",
    "\n",
    "def d_composite(x):\n",
    "    # Chain rule: d/dx(outer(inner(x))) = d(outer)/d(inner) * d(inner)/dx\n",
    "    return np.exp(x**3) * 3*x**2\n",
    "\n",
    "x = np.linspace(0, 2, 100)\n",
    "y = composite(x)\n",
    "dy = d_composite(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='h(x) = exp(x^3)')\n",
    "plt.plot(x, dy, label=\"h'(x) = 3x^2 * exp(x^3)\")\n",
    "plt.legend()\n",
    "plt.title('Composite Function and its Derivative')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Derivatives with Multiple Inputs\n",
    "\n",
    "In machine learning, we often work with functions that have multiple inputs. The partial derivatives of these functions form the gradient, which is crucial for optimization algorithms.\n",
    "\n",
    "For a function $f(x, y)$, the gradient is defined as:\n",
    "\n",
    "$$\\nabla f(x, y) = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)$$\n",
    "\n",
    "Let's look at a simple function with two inputs and its gradient:\n",
    "\n",
    "$$f(x, y) = x^2 + y^2$$\n",
    "$$\\nabla f(x, y) = (2x, 2y)$$\n",
    "\n",
    "Here's a visualization of this function and its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def grad_f(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "x = np.linspace(-2, 2, 20)\n",
    "y = np.linspace(-2, 2, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X, Y)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(X, Y, Z, cmap='viridis')\n",
    "ax1.set_title('f(x, y) = x^2 + y^2')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "U, V = grad_f(X, Y)\n",
    "ax2.quiver(X, Y, U, V)\n",
    "ax2.set_title('Gradient of f(x, y)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Derivatives with Multiple Vector Inputs\n",
    "\n",
    "In deep learning, we often deal with functions that take multiple vectors as inputs. The Jacobian matrix represents the derivatives of these functions.\n",
    "\n",
    "For a function $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian matrix is defined as:\n",
    "\n",
    "$$J = \\begin{bmatrix}\n",
    "    \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Let's implement a simple neural network layer and compute its Jacobian. We'll use the sigmoid activation function:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "The layer function is defined as:\n",
    "\n",
    "$$\\mathbf{y} = \\sigma(W\\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "Where $W$ is the weight matrix and $\\mathbf{b}$ is the bias vector.\n",
    "\n",
    "The Jacobian of this layer with respect to the input $\\mathbf{x}$ is:\n",
    "\n",
    "$$J = \\text{diag}(\\sigma(W\\mathbf{x} + \\mathbf{b}) \\odot (1 - \\sigma(W\\mathbf{x} + \\mathbf{b}))) \\cdot W$$\n",
    "\n",
    "Where $\\odot$ represents element-wise multiplication.\n",
    "\n",
    "Here's the implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def layer(x, W, b):\n",
    "    return sigmoid(np.dot(W, x) + b)\n",
    "\n",
    "def jacobian(x, W, b):\n",
    "    z = np.dot(W, x) + b\n",
    "    s = sigmoid(z)\n",
    "    return np.diag(s * (1 - s)) @ W\n",
    "\n",
    "# Example usage\n",
    "x = np.array([1, 2, 3])\n",
    "W = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
    "b = np.array([0.1, 0.2])\n",
    "\n",
    "print(\"Layer output:\")\n",
    "print(layer(x, W, b))\n",
    "\n",
    "print(\"\\nJacobian:\")\n",
    "print(jacobian(x, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various aspects of derivatives and their applications in machine learning. We've covered basic derivative functions, nested functions, the chain rule, derivatives with multiple inputs, and derivatives with multiple vector inputs.\n",
    "\n",
    "Understanding these concepts is crucial for:\n",
    "1. Implementing gradient descent and other optimization algorithms\n",
    "2. Designing and training neural networks\n",
    "3. Understanding backpropagation\n",
    "4. Analyzing the sensitivity of models to their inputs and parameters\n",
    "\n",
    "As you continue your journey in machine learning and deep learning, you'll find these concepts appearing repeatedly, forming the foundation of many advanced techniques and algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
